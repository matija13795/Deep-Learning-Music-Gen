{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae954cc",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the ABC Music Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce08da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tunes extracted: 1049\n",
      "Example tune:\n",
      "\n",
      "X: 1\n",
      "T:A and A's Waltz\n",
      "% Nottingham Music Database\n",
      "S:Mick Peat\n",
      "M:3/4\n",
      "L:1/4\n",
      "K:G\n",
      "e|:\"G\"d2B|\"D\"A3/2B/2c|\"G\"B2G|\"D\"A2e|\"G\"d2B|\"D\"A3/2B/2c|\n",
      "M:2/4\n",
      "\"F\"B=F|\n",
      "M:3/4\n",
      "\"G\"G2e:||:\n",
      "\"C\"g2e|\"Bb\"=f2d|\"F\"c2A|=F2e|\"C\"g2e|\"Bb\"=f2d|\n",
      "M:2/4\n",
      "\"F\"cA|\n",
      "M:3/4\n",
      " [1 \"G\"G2e:| [2\"G\"G2z||\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"data/\"\n",
    "abc_files = [f for f in os.listdir(data_dir) if f.endswith(\".abc\")]\n",
    "\n",
    "# Each .abc file contains multiple songs. We will split each file into a list of individual tunes.\n",
    "all_tunes = []\n",
    "for filename in abc_files:\n",
    "    with open(os.path.join(data_dir, filename), 'r') as file:\n",
    "        content = file.read()\n",
    "        # In a file, each song is separated by a blank line\n",
    "        tunes = content.split('\\n\\n')\n",
    "        for i, tune in enumerate(tunes):\n",
    "            all_tunes.append(tune)\n",
    "\n",
    "print(f\"Total tunes extracted: {len(all_tunes)}\")\n",
    "print(\"Example tune:\")\n",
    "print(all_tunes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2da5a6",
   "metadata": {},
   "source": [
    "Some tunes may be incomplete or junk. A simple heuristic:\n",
    "- Keep only tunes with a key signature K:\n",
    "- Maybe also require a time signature M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af0d2394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunes after cleaning: 1034\n"
     ]
    }
   ],
   "source": [
    "clean_tunes = [t for t in all_tunes if 'K:' in t and 'M:' in t]\n",
    "print(f\"Tunes after cleaning: {len(clean_tunes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9b97da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example melody:\n",
      "M:6/8 L:1/8 K:D e|:\"D\"dcd \"A\"FAB|\"D\"dcd \"G\"BAG|\"D\"FAd \"G\"GBd|\"Em\"Ged \"A\"cBc| \"D\"dcd \"A\"FAB|\"D\"dcd \"G\"B2d|\"Em\"Bed \"A\"cag|\"D\"fdc d2e:| |:\"D\"fdA \"G\"g3|\"C\"e=cG \"F\"=f3|\"C\"e=cG Gce|\"C\"=ceg \"G\"Bdg| \"D\"fdA \"G\"g3|\"A\"ecA \"D\"f3|\"Bm\"def \"Em\"ged|\"A\"cBc \"D\"d3:|\n"
     ]
    }
   ],
   "source": [
    "def extract_melody_with_context(tune):\n",
    "    \"\"\"\n",
    "    Extracts the essential musical information and melody from an ABC tune.\n",
    "    Automatically fills in a default L: value if missing, based on M: (meter).\n",
    "\n",
    "    Parameters:\n",
    "        tune (str): A raw ABC tune as a multiline string.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing only the essential musical context and the melody.\n",
    "\n",
    "    Notes:\n",
    "        In ABC notation, tunes begin with a header section that may contain many fields.\n",
    "        Only some of these fields are musically relevant to interpreting the notes.\n",
    "\n",
    "        We keep:\n",
    "            - M: Meter (e.g., \"M:4/4\")\n",
    "            - L: Default note length (e.g., \"L:1/8\")\n",
    "            - K: Key signature (e.g., \"K:Cmaj\")\n",
    "\n",
    "        These fields are needed to interpret rhythm and pitch correctly.\n",
    "\n",
    "        We discard:\n",
    "            - X: Tune number (just an ID)\n",
    "            - T: Title\n",
    "            - N:, C:, Z:, Q:, etc. — any fields that are comments, composer names,\n",
    "              tempo hints, etc., which are often inconsistent or irrelevant for modeling.\n",
    "\n",
    "        Once we reach the 'K:' line (the key signature), we start including melody lines,\n",
    "        which contain the actual note sequences.\n",
    "\n",
    "        Rules for default L:\n",
    "        - If no L: line is provided, compute decimal value of the meter (M:)\n",
    "        - If meter >= 0.75 → default to L:1/8\n",
    "        - If meter <  0.75 → default to L:1/16\n",
    "    \"\"\"\n",
    "    lines = tune.strip().splitlines()\n",
    "\n",
    "    meter_line = None\n",
    "    length_line = None\n",
    "    key_line = None\n",
    "    melody_lines = []\n",
    "\n",
    "    header_done = False\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if not header_done:\n",
    "            if line.startswith(\"M:\") and meter_line is None:\n",
    "                meter_line = line\n",
    "            elif line.startswith(\"L:\") and length_line is None:\n",
    "                length_line = line\n",
    "            elif line.startswith(\"K:\") and key_line is None:\n",
    "                key_line = line\n",
    "                header_done = True  # everything after this is melody\n",
    "        else:\n",
    "            melody_lines.append(line)\n",
    "\n",
    "    # Synthesize L: if missing\n",
    "    if length_line is None:\n",
    "        default_length = \"1/8\"  # fallback\n",
    "        if meter_line:\n",
    "            try:\n",
    "                meter_value = meter_line[2:].strip()\n",
    "                num, denom = map(int, meter_value.split('/'))\n",
    "                meter_decimal = num / denom\n",
    "                default_length = \"1/8\" if meter_decimal >= 0.75 else \"1/16\"\n",
    "            except Exception:\n",
    "                pass\n",
    "        length_line = f\"L:{default_length}\"\n",
    "\n",
    "    header = []\n",
    "    if meter_line:\n",
    "        header.append(meter_line)\n",
    "    if length_line:\n",
    "        header.append(length_line)\n",
    "    if key_line:\n",
    "        header.append(key_line)\n",
    "\n",
    "    return ' '.join(header + melody_lines)\n",
    "\n",
    "\n",
    "melodies = [extract_melody_with_context(tune) for tune in clean_tunes]\n",
    "print(\"Example melody:\")\n",
    "print(melodies[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d303372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_slash_to_half(abc_str):\n",
    "    \"\"\"\n",
    "    Replaces note durations written with a dangling '/' (e.g., 'b/') \n",
    "    with '/2' to standardize fractional durations.\n",
    "    \"\"\"\n",
    "    return re.sub(r'([=^_]*[A-Ga-grz][\\',]?)\\/(?!\\d)', r'\\1/2', abc_str)\n",
    "\n",
    "def normalize_chord_formatting(abc_str):\n",
    "    \"\"\"\n",
    "    Cleans up chord notation:\n",
    "    - Removes parentheses around chords (e.g., \"(A7)\" → \"A7\")\n",
    "    - Fixes space typos (e.g., \"D m\" → \"Dm\")\n",
    "    - Strips leading/trailing whitespace inside quotes (e.g., '\" Em\"' → '\"Em\"')\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_chord(match):\n",
    "        chord = match.group(0)\n",
    "        chord = chord.strip('\"() ')       # remove outer symbols and spaces\n",
    "        chord = chord.replace(\" \", \"\")    # squash space typos like \"D m\"\n",
    "        return f'\"{chord}\"'\n",
    "\n",
    "    return re.sub(r'\"[^\"]+\"', clean_chord, abc_str)\n",
    "\n",
    "def preprocess_abc(abc_str):\n",
    "    abc_str = normalize_slash_to_half(abc_str)\n",
    "    abc_str = normalize_chord_formatting(abc_str)\n",
    "    return abc_str\n",
    "\n",
    "normalized_melodies = [preprocess_abc(melody) for melody in melodies]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a1524",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1787d1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M:3/4', 'L:1/4', 'K:G', 'e', '|:', '\"G\"', 'd2', 'B', '|', '\"D\"', 'A3/2', 'B/2', 'c', '|', '\"G\"', 'B2', 'G', '|', '\"D\"', 'A2', 'e', '|', '\"G\"', 'd2', 'B', '|', '\"D\"', 'A3/2', 'B/2', 'c', '|', 'M:2/4', '\"F\"', 'B', '=F', '|', 'M:3/4', '\"G\"', 'G2', 'e', ':||', '\"C\"', 'g2', 'e', '|', '\"Bb\"', '=f2', 'd', '|', '\"F\"', 'c2', 'A', '|', '=F2', 'e', '|', '\"C\"', 'g2', 'e', '|', '\"Bb\"', '=f2', 'd', '|', 'M:2/4', '\"F\"', 'c', 'A', '|', 'M:3/4', '[1', '\"G\"', 'G2', 'e', ':|', '[2', '\"G\"', 'G2', 'z', '||']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_abc(abc_str):\n",
    "    \"\"\"\n",
    "    Tokenizes an ABC notation string into musically meaningful symbols.\n",
    "\n",
    "    The tokenizer extracts:\n",
    "        - Bar lines and repeat symbols: '|', '||', ':|', '|:', '::'\n",
    "        - Repeat brackets: '[1', '[2'\n",
    "        - Chords: enclosed in double quotes, e.g., \"G\", \"Dmin\"\n",
    "        - Notes: including accidentals (=, ^, _), octave markers (',), rests (z), and durations (e.g., A3/2)\n",
    "        - Headers: \n",
    "            - M: (time signature), e.g., M:4/4\n",
    "            - L: (default note length), e.g., L:1/8\n",
    "            - K: (key signature), e.g., K:D#\n",
    "\n",
    "    Parameters:\n",
    "        abc_str (str): A string in ABC notation.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of tokens representing the music in ABC format.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = re.findall(r'''\n",
    "          \\[1|\\[2                          # repeat brackets [1, [2\n",
    "        | :\\|\\|                            # :|| (repeat + end)\n",
    "        | :\\|                              # :|  (end of repeat)\n",
    "        | \\|:                              # |:  (start of repeat)\n",
    "        | ::                               # ::  (double repeat)\n",
    "        | \\|\\|                             # ||  (end of section)\n",
    "        | \\|                               # |   (single bar)\n",
    "        | \\|\\||\\|                          # barlines\n",
    "        | \"[^\"]+\"                          # chords, e.g., \"G\", \"Dmin\"\n",
    "        | [=^_]*[A-Ga-grz][\\',]?\\d*\\/?\\d*  # notes (d2, A3/2, z, etc)\n",
    "        | M:\\d+\\/\\d+                       # time signature\n",
    "        | L:\\d+\\/\\d+                       # default note length\n",
    "        | K:[A-G][#b]?m?                   # key signature\n",
    "        ''',\n",
    "        abc_str,\n",
    "        re.VERBOSE\n",
    "    )\n",
    "    return tokens\n",
    "\n",
    "tokenized_melodies = [tokenize_abc(melody) for melody in normalized_melodies]\n",
    "tokens = tokenize_abc(melodies[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd224d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS] Test 1\n",
      "[PASS] Test 2\n",
      "[PASS] Test 3\n",
      "[PASS] Test 4\n"
     ]
    }
   ],
   "source": [
    "def run_tokenizer_tests():\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"input\": 'M:4/4 L:1/8 K:C CDEF GABc|',\n",
    "            \"expected\": ['M:4/4', 'L:1/8', 'K:C', 'C', 'D', 'E', 'F', 'G', 'A', 'B', 'c', '|']\n",
    "        },\n",
    "        {\n",
    "            \"input\": 'K:G \"D7\" D2 G2 |: B4 :|',\n",
    "            \"expected\": ['K:G', '\"D7\"', 'D2', 'G2', '|:', 'B4', ':|']\n",
    "        },\n",
    "        {\n",
    "            \"input\": 'L:1/16 M:6/8 K:D z3 [1 A2 B2 :||',\n",
    "            \"expected\": ['L:1/16', 'M:6/8', 'K:D', 'z3', '[1', 'A2', 'B2', ':||']\n",
    "        },\n",
    "        {\n",
    "            \"input\": 'K:D | A3/2 B/2 c\\'/ | \"G\" G,2 |',\n",
    "            \"expected\": ['K:D', '|', 'A3/2', 'B/2', \"c'/\", '|', '\"G\"', 'G,2', '|']\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for i, case in enumerate(test_cases, 1):\n",
    "        result = tokenize_abc(case['input'])\n",
    "        if result != case['expected']:\n",
    "            print(f\"[FAIL] Test {i}\")\n",
    "            print(\"Input:   \", case['input'])\n",
    "            print(\"Expected:\", case['expected'])\n",
    "            print(\"Got:     \", result)\n",
    "        else:\n",
    "            print(f\"[PASS] Test {i}\")\n",
    "\n",
    "run_tokenizer_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba55d6",
   "metadata": {},
   "source": [
    "## Building a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b75e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 440\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(tokenized_melodies):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary (token-to-index mapping) from a list of tokenized ABC melodies.\n",
    "\n",
    "    Parameters:\n",
    "        tokenized_melodies (List[List[str]]): A list where each melody is a list of ABC tokens.\n",
    "\n",
    "    Returns:\n",
    "        vocab (Dict[str, int]): Mapping from tokens to unique integer indices.\n",
    "        indexed_melodies (List[List[int]]): Each melody as a list of integer token indices.\n",
    "        token_freq (Counter): Frequency count of all tokens.\n",
    "    \"\"\"\n",
    "    # Flatten all tokenized melodies into a single list of tokens\n",
    "    all_tokens = [token for melody in tokenized_melodies for token in melody]\n",
    "\n",
    "    # Count how often each token appears\n",
    "    token_freq = Counter(all_tokens)\n",
    "\n",
    "    # Create vocabulary: map each unique token to a unique index\n",
    "    # Start from 1 so that 0 can be reserved for padding\n",
    "    vocab = {token: idx for idx, (token, _) in enumerate(token_freq.items(), start=1)}\n",
    "    vocab[\"<PAD>\"] = 0  # Add padding token (index 0)\n",
    "\n",
    "    # Convert each tokenized melody to a list of token indices\n",
    "    indexed_melodies = [[vocab[token] for token in melody] for melody in tokenized_melodies]\n",
    "\n",
    "    return vocab, indexed_melodies, token_freq\n",
    "\n",
    "\n",
    "# Build vocab from your tokenized melodies\n",
    "vocab, indexed_melodies, token_freq = build_vocab(tokenized_melodies)\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "print(f\"Unique tokens: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4418dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TimeSig (9 tokens) ---\n",
      "M:2/2\n",
      "M:2/4\n",
      "M:3/2\n",
      "M:3/4\n",
      "M:4/4\n",
      "M:5/4\n",
      "M:6/4\n",
      "M:6/8\n",
      "M:9/8\n",
      "\n",
      "--- NoteLength (2 tokens) ---\n",
      "L:1/4\n",
      "L:1/8\n",
      "\n",
      "--- Key (14 tokens) ---\n",
      "K:A\n",
      "K:Am\n",
      "K:B\n",
      "K:Bb\n",
      "K:Bm\n",
      "K:C\n",
      "K:Cm\n",
      "K:D\n",
      "K:Dm\n",
      "K:E\n",
      "K:Em\n",
      "K:F\n",
      "K:G\n",
      "K:Gm\n",
      "\n",
      "--- Notes (298 tokens) ---\n",
      "=A\n",
      "=A/2\n",
      "=B\n",
      "=B,\n",
      "=B/2\n",
      "=B2\n",
      "=B3\n",
      "=C\n",
      "=C/2\n",
      "=D\n",
      "=D/2\n",
      "=E\n",
      "=E/2\n",
      "=E/4\n",
      "=F\n",
      "=F/2\n",
      "=F2\n",
      "=F3\n",
      "=G\n",
      "=G/2\n",
      "=G/4\n",
      "=G2\n",
      "=G3/2\n",
      "=a/2\n",
      "=a2\n",
      "=b/2\n",
      "=c\n",
      "=c'2\n",
      "=c/2\n",
      "=c/4\n",
      "=c2\n",
      "=c3\n",
      "=c3/2\n",
      "=c3/4\n",
      "=c4\n",
      "=d\n",
      "=d/2\n",
      "=e\n",
      "=e/2\n",
      "=f\n",
      "=f/2\n",
      "=f2\n",
      "=f3\n",
      "=f3/2\n",
      "=f4\n",
      "=g\n",
      "=g/2\n",
      "=g2\n",
      "A\n",
      "A,\n",
      "A,/2\n",
      "A,2\n",
      "A,3\n",
      "A,4\n",
      "A/2\n",
      "A/4\n",
      "A2\n",
      "A3\n",
      "A3/2\n",
      "A3/4\n",
      "A4\n",
      "A6\n",
      "B\n",
      "B,\n",
      "B,/2\n",
      "B,/4\n",
      "B,2\n",
      "B,3\n",
      "B,3/2\n",
      "B,6\n",
      "B/2\n",
      "B/4\n",
      "B2\n",
      "B3\n",
      "B3/2\n",
      "B3/4\n",
      "B4\n",
      "B5\n",
      "B6\n",
      "C\n",
      "C/2\n",
      "C/4\n",
      "C2\n",
      "C3\n",
      "C3/2\n",
      "C4\n",
      "C6\n",
      "D\n",
      "D/2\n",
      "D/4\n",
      "D2\n",
      "D3\n",
      "D3/2\n",
      "D3/4\n",
      "D4\n",
      "D6\n",
      "E\n",
      "E/2\n",
      "E/4\n",
      "E2\n",
      "E3\n",
      "E3/2\n",
      "E3/4\n",
      "E4\n",
      "E6\n",
      "F\n",
      "F/2\n",
      "F/4\n",
      "F2\n",
      "F3\n",
      "F3/2\n",
      "F3/4\n",
      "F4\n",
      "F6\n",
      "G\n",
      "G,\n",
      "G,/2\n",
      "G,2\n",
      "G,3\n",
      "G/2\n",
      "G/4\n",
      "G2\n",
      "G3\n",
      "G3/2\n",
      "G3/4\n",
      "G4\n",
      "G6\n",
      "^A\n",
      "^A,\n",
      "^A/2\n",
      "^A/4\n",
      "^A2\n",
      "^A3\n",
      "^A3/2\n",
      "^B\n",
      "^B,/2\n",
      "^B/2\n",
      "^B2\n",
      "^C\n",
      "^C/2\n",
      "^C2\n",
      "^C3\n",
      "^D\n",
      "^D/2\n",
      "^D2\n",
      "^D3\n",
      "^D3/2\n",
      "^E\n",
      "^F\n",
      "^F/2\n",
      "^F/4\n",
      "^F2\n",
      "^F3\n",
      "^G\n",
      "^G/2\n",
      "^G/4\n",
      "^G2\n",
      "^G3\n",
      "^G3/2\n",
      "^G3/4\n",
      "^a\n",
      "^a/2\n",
      "^c\n",
      "^c/2\n",
      "^c/4\n",
      "^c2\n",
      "^c3\n",
      "^c3/2\n",
      "^c3/4\n",
      "^d\n",
      "^d/2\n",
      "^d/4\n",
      "^d2\n",
      "^d3\n",
      "^d3/2\n",
      "^e\n",
      "^e/2\n",
      "^e2\n",
      "^e3\n",
      "^f\n",
      "^f/2\n",
      "^f2\n",
      "^f3\n",
      "^f3/4\n",
      "^g\n",
      "^g/2\n",
      "^g/4\n",
      "^g2\n",
      "_A\n",
      "_B\n",
      "_B/2\n",
      "_B2\n",
      "_B3\n",
      "_D\n",
      "_E\n",
      "_E/2\n",
      "_E2\n",
      "_E3\n",
      "_F3\n",
      "_G\n",
      "_a\n",
      "_b\n",
      "_b/2\n",
      "_b2\n",
      "_b3/2\n",
      "_c\n",
      "_d\n",
      "_e\n",
      "_e/2\n",
      "_e2\n",
      "_e3\n",
      "_f\n",
      "_g\n",
      "_g2\n",
      "a\n",
      "a/2\n",
      "a/4\n",
      "a/8\n",
      "a2\n",
      "a3\n",
      "a3/2\n",
      "a3/4\n",
      "a4\n",
      "a6\n",
      "b\n",
      "b/2\n",
      "b/4\n",
      "b2\n",
      "b3\n",
      "b3/2\n",
      "b3/4\n",
      "b4\n",
      "c\n",
      "c'\n",
      "c'/2\n",
      "c'2\n",
      "c'3\n",
      "c'3/2\n",
      "c/2\n",
      "c/4\n",
      "c2\n",
      "c3\n",
      "c3/2\n",
      "c3/4\n",
      "c4\n",
      "c6\n",
      "d\n",
      "d'\n",
      "d'2\n",
      "d'3/2\n",
      "d/2\n",
      "d/4\n",
      "d2\n",
      "d3\n",
      "d3/2\n",
      "d3/4\n",
      "d4\n",
      "d6\n",
      "e\n",
      "e'\n",
      "e'2\n",
      "e/2\n",
      "e/4\n",
      "e/8\n",
      "e2\n",
      "e3\n",
      "e3/2\n",
      "e3/4\n",
      "e4\n",
      "e6\n",
      "f\n",
      "f/2\n",
      "f/4\n",
      "f/8\n",
      "f2\n",
      "f3\n",
      "f3/2\n",
      "f3/4\n",
      "f4\n",
      "f6\n",
      "g\n",
      "g/2\n",
      "g/4\n",
      "g/8\n",
      "g2\n",
      "g3\n",
      "g3/2\n",
      "g3/4\n",
      "g4\n",
      "g6\n",
      "r\n",
      "z\n",
      "z/2\n",
      "z/4\n",
      "z2\n",
      "z3\n",
      "z3/2\n",
      "z4\n",
      "\n",
      "--- Barlines (6 tokens) ---\n",
      "::\n",
      ":|\n",
      ":||\n",
      "|\n",
      "|:\n",
      "||\n",
      "\n",
      "--- Chords (108 tokens) ---\n",
      "\"A\"\n",
      "\"A/2c+\"\n",
      "\"A/2e\"\n",
      "\"A7\"\n",
      "\"A7/c+\"\n",
      "\"A7/e\"\n",
      "\"A7/f+\"\n",
      "\"A7/g\"\n",
      "\"Ab\"\n",
      "\"Ad\"\n",
      "\"Am\"\n",
      "\"Am/c\"\n",
      "\"Am/e\"\n",
      "\"Am/f+\"\n",
      "\"Am/g\"\n",
      "\"Am/g+\"\n",
      "\"Am7\"\n",
      "\"Am7/g\"\n",
      "\"B\"\n",
      "\"B7\"\n",
      "\"B7/f+\"\n",
      "\"Bb\"\n",
      "\"Bb/2d\"\n",
      "\"Bb/2f\"\n",
      "\"Bb7\"\n",
      "\"Bbd\"\n",
      "\"Bm\"\n",
      "\"Bm/a\"\n",
      "\"Bm/d\"\n",
      "\"Bm/f+\"\n",
      "\"Bm7\"\n",
      "\"C\"\n",
      "\"C#\"\n",
      "\"C#7\"\n",
      "\"C#d\"\n",
      "\"C#m\"\n",
      "\"C/2c\"\n",
      "\"C/2e\"\n",
      "\"C/2g\"\n",
      "\"C6\"\n",
      "\"C7\"\n",
      "\"C7/g\"\n",
      "\"Cd\"\n",
      "\"Cm\"\n",
      "\"Cm/g\"\n",
      "\"Cm6\"\n",
      "\"D\"\n",
      "\"D#d\"\n",
      "\"D/2a\"\n",
      "\"D/2c+\"\n",
      "\"D/2f+\"\n",
      "\"D/2g\"\n",
      "\"D6\"\n",
      "\"D7\"\n",
      "\"D7/a\"\n",
      "\"D7/b\"\n",
      "\"D7/c\"\n",
      "\"D7/f+\"\n",
      "\"D7b9\"\n",
      "\"Da\"\n",
      "\"Dd\"\n",
      "\"Dm\"\n",
      "\"Dm/a\"\n",
      "\"Dm/f\"\n",
      "\"Dm6\"\n",
      "\"Dm7/c\"\n",
      "\"E\"\n",
      "\"E7\"\n",
      "\"E7/b\"\n",
      "\"E7/g+\"\n",
      "\"E7b9\"\n",
      "\"Ea\"\n",
      "\"Eb\"\n",
      "\"Ebd\"\n",
      "\"Ed\"\n",
      "\"Em\"\n",
      "\"Em/c+\"\n",
      "\"Em/d\"\n",
      "\"Em/d+\"\n",
      "\"Em/g\"\n",
      "\"Em7\"\n",
      "\"Em7/d\"\n",
      "\"F\"\n",
      "\"F#\"\n",
      "\"F#7\"\n",
      "\"F#m\"\n",
      "\"F#m/a\"\n",
      "\"F/2a\"\n",
      "\"F/2c\"\n",
      "\"F7\"\n",
      "\"F7/a\"\n",
      "\"Fa7\"\n",
      "\"Fd\"\n",
      "\"Fm\"\n",
      "\"G\"\n",
      "\"G#7\"\n",
      "\"G#d\"\n",
      "\"G/2b\"\n",
      "\"G/2d\"\n",
      "\"G6\"\n",
      "\"G7\"\n",
      "\"G7/b\"\n",
      "\"G7/d\"\n",
      "\"G7/f\"\n",
      "\"Gd\"\n",
      "\"Gm\"\n",
      "\"Gm/bb\"\n",
      "\"Gm/d\"\n",
      "\n",
      "--- RepeatBrackets (2 tokens) ---\n",
      "[1\n",
      "[2\n",
      "\n",
      "--- Special (1 tokens) ---\n",
      "<PAD>\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group tokens into categories\n",
    "grouped_tokens = defaultdict(list)\n",
    "\n",
    "for token in vocab:\n",
    "    if token == \"<PAD>\":\n",
    "        grouped_tokens[\"Special\"].append(token)\n",
    "    elif re.match(r'^K:[A-G][#b]?m?$', token):\n",
    "        grouped_tokens[\"Key\"].append(token)\n",
    "    elif re.match(r'^M:\\d+/\\d+$', token):\n",
    "        grouped_tokens[\"TimeSig\"].append(token)\n",
    "    elif re.match(r'^L:\\d+/\\d+$', token):\n",
    "        grouped_tokens[\"NoteLength\"].append(token)\n",
    "    elif re.match(r'^\"[^\"]+\"$', token):\n",
    "        grouped_tokens[\"Chords\"].append(token)\n",
    "    elif re.match(r'^\\[1|\\[2$', token):\n",
    "        grouped_tokens[\"RepeatBrackets\"].append(token)\n",
    "    elif re.match(r'^::|:\\|\\||\\|:|:\\||\\|\\||\\|$', token):\n",
    "        grouped_tokens[\"Barlines\"].append(token)\n",
    "    elif re.match(r'^[=^_]*[A-Ga-grz][\\',]?\\d*/?\\d*$', token):\n",
    "        grouped_tokens[\"Notes\"].append(token)\n",
    "    else:\n",
    "        grouped_tokens[\"Other/Weird\"].append(token)\n",
    "\n",
    "# Print grouped tokens\n",
    "for group, tokens in grouped_tokens.items():\n",
    "    print(f\"\\n--- {group} ({len(tokens)} tokens) ---\")\n",
    "    for token in sorted(tokens):\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a3634",
   "metadata": {},
   "source": [
    "## Creating a PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3626dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NottinghamDataset(Dataset):\n",
    "    def __init__(self, indexed_sequences, vocab, inv_vocab, window_size=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            indexed_sequences (List[List[int]]): List of tokenized melody sequences (as integers).\n",
    "            vocab (Dict[str, int]): Token-to-index mapping.\n",
    "            inv_vocab (Dict[int, str]): Index-to-token mapping.\n",
    "            window_size (int): Number of tokens to include in each input window.\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.vocab = vocab\n",
    "        self.inv_vocab = inv_vocab\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Go through each melody and turn it into sliding input windows\n",
    "        for seq in indexed_sequences:\n",
    "            context = self.extract_context(seq)  # this grabs the first 3 tokens (assumed M:/L:/K:)\n",
    "            body = seq[3:]  # remove M:/L:/K: from melody body\n",
    "            \n",
    "            windows, labels = self.generate_windows(body, context)\n",
    "            self.inputs.extend(windows)\n",
    "            self.labels.extend(labels)\n",
    "\n",
    "    def extract_context(self, seq):\n",
    "        return seq[:3]  # assuming M:/L:/K: are always at the beginning\n",
    "\n",
    "    def generate_windows(self, seq, context):\n",
    "        windows = []\n",
    "        labels = []\n",
    "        for i in range(len(seq) - self.window_size):\n",
    "            input_window = seq[i:i+self.window_size]\n",
    "            target_token = seq[i+self.window_size]\n",
    "            full_input = context + input_window\n",
    "            windows.append(torch.tensor(full_input, dtype=torch.long))\n",
    "            labels.append(torch.tensor(target_token, dtype=torch.long))\n",
    "        return windows, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "WINDOW_SIZE = 16\n",
    "dataset = NottinghamDataset(indexed_melodies, vocab, inv_vocab, WINDOW_SIZE)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_ds, val_ds, test_ds = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "148e76bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141585\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "657a6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1024)\n",
    "test_loader = DataLoader(test_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca980e",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We’ll use a simple feedforward neural network:\n",
    "- Inputs are token IDs (integers), so we first use an embedding layer to convert each token into a vector.\n",
    "- Then we flatten the embedded input (which is a 2D sequence) into a 1D vector.\n",
    "- Then feed it through one fully connected layers.\n",
    "- Final output is a vector of length equal to vocab_size, representing logits for each possible next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b795324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MelodyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear((WINDOW_SIZE + 3) * embed_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)  # output logits for all tokens\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        x = self.embedding(x)  # -> (batch_size, seq_len, embed_dim)\n",
    "        x = x.view(x.size(0), -1)  # flatten: (batch_size, seq_len * embed_dim)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # logits (no softmax here since crossentropy expects logits)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9638789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MelodyModel(vocab_size=len(vocab), embed_dim=128, hidden_dim=512, dropout=0.5)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eef6b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2add4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = loss_fn(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b16e0d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 3.1084, Val Loss = 2.5784\n",
      "Epoch 2: Train Loss = 2.5194, Val Loss = 2.3888\n",
      "Epoch 3: Train Loss = 2.3320, Val Loss = 2.2881\n",
      "Epoch 4: Train Loss = 2.2109, Val Loss = 2.2246\n",
      "Epoch 5: Train Loss = 2.1210, Val Loss = 2.1710\n",
      "Epoch 6: Train Loss = 2.0478, Val Loss = 2.1369\n",
      "Epoch 7: Train Loss = 1.9913, Val Loss = 2.1034\n",
      "Epoch 8: Train Loss = 1.9373, Val Loss = 2.0779\n",
      "Epoch 9: Train Loss = 1.9032, Val Loss = 2.0613\n",
      "Epoch 10: Train Loss = 1.8659, Val Loss = 2.0486\n",
      "Epoch 11: Train Loss = 1.8344, Val Loss = 2.0355\n",
      "Epoch 12: Train Loss = 1.8151, Val Loss = 2.0198\n",
      "Epoch 13: Train Loss = 1.7920, Val Loss = 2.0115\n",
      "Epoch 14: Train Loss = 1.7763, Val Loss = 1.9970\n",
      "Epoch 15: Train Loss = 1.7609, Val Loss = 1.9908\n",
      "Epoch 16: Train Loss = 1.7499, Val Loss = 1.9892\n",
      "Epoch 17: Train Loss = 1.7341, Val Loss = 1.9806\n",
      "Epoch 18: Train Loss = 1.7276, Val Loss = 1.9737\n",
      "Epoch 19: Train Loss = 1.7179, Val Loss = 1.9700\n",
      "Epoch 20: Train Loss = 1.7125, Val Loss = 1.9657\n",
      "Epoch 21: Train Loss = 1.7056, Val Loss = 1.9628\n",
      "Epoch 22: Train Loss = 1.7011, Val Loss = 1.9590\n",
      "Epoch 23: Train Loss = 1.6942, Val Loss = 1.9578\n",
      "Epoch 24: Train Loss = 1.6880, Val Loss = 1.9580\n",
      "Epoch 25: Train Loss = 1.6840, Val Loss = 1.9540\n",
      "Epoch 26: Train Loss = 1.6804, Val Loss = 1.9510\n",
      "Epoch 27: Train Loss = 1.6748, Val Loss = 1.9454\n",
      "Epoch 28: Train Loss = 1.6691, Val Loss = 1.9405\n",
      "Epoch 29: Train Loss = 1.6644, Val Loss = 1.9397\n",
      "Epoch 30: Train Loss = 1.6604, Val Loss = 1.9435\n",
      "Epoch 31: Train Loss = 1.6541, Val Loss = 1.9379\n",
      "Epoch 32: Train Loss = 1.6508, Val Loss = 1.9350\n",
      "Epoch 33: Train Loss = 1.6469, Val Loss = 1.9342\n",
      "Epoch 34: Train Loss = 1.6390, Val Loss = 1.9318\n",
      "Epoch 35: Train Loss = 1.6361, Val Loss = 1.9341\n",
      "Epoch 36: Train Loss = 1.6287, Val Loss = 1.9280\n",
      "Epoch 37: Train Loss = 1.6270, Val Loss = 1.9264\n",
      "Epoch 38: Train Loss = 1.6258, Val Loss = 1.9218\n",
      "Epoch 39: Train Loss = 1.6174, Val Loss = 1.9184\n",
      "Epoch 40: Train Loss = 1.6147, Val Loss = 1.9207\n",
      "Epoch 41: Train Loss = 1.6097, Val Loss = 1.9229\n",
      "Epoch 42: Train Loss = 1.6016, Val Loss = 1.9232\n",
      "Epoch 43: Train Loss = 1.5975, Val Loss = 1.9169\n",
      "Epoch 44: Train Loss = 1.5944, Val Loss = 1.9140\n",
      "Epoch 45: Train Loss = 1.5906, Val Loss = 1.9146\n",
      "Epoch 46: Train Loss = 1.5849, Val Loss = 1.9141\n",
      "Epoch 47: Train Loss = 1.5805, Val Loss = 1.9141\n",
      "Epoch 48: Train Loss = 1.5763, Val Loss = 1.9048\n",
      "Epoch 49: Train Loss = 1.5724, Val Loss = 1.9074\n",
      "Epoch 50: Train Loss = 1.5698, Val Loss = 1.9108\n",
      "Epoch 51: Train Loss = 1.5618, Val Loss = 1.9104\n",
      "Epoch 52: Train Loss = 1.5589, Val Loss = 1.9125\n",
      "Epoch 53: Train Loss = 1.5585, Val Loss = 1.9019\n",
      "Epoch 54: Train Loss = 1.5520, Val Loss = 1.9021\n",
      "Epoch 55: Train Loss = 1.5494, Val Loss = 1.9061\n",
      "Epoch 56: Train Loss = 1.5446, Val Loss = 1.9032\n",
      "Epoch 57: Train Loss = 1.5406, Val Loss = 1.9042\n",
      "Epoch 58: Train Loss = 1.5458, Val Loss = 1.9044\n",
      "Epoch 59: Train Loss = 1.5369, Val Loss = 1.9023\n",
      "Epoch 60: Train Loss = 1.5312, Val Loss = 1.9028\n",
      "Epoch 61: Train Loss = 1.5305, Val Loss = 1.8996\n",
      "Epoch 62: Train Loss = 1.5252, Val Loss = 1.9021\n",
      "Epoch 63: Train Loss = 1.5214, Val Loss = 1.9021\n",
      "Epoch 64: Train Loss = 1.5223, Val Loss = 1.8977\n",
      "Epoch 65: Train Loss = 1.5135, Val Loss = 1.8996\n",
      "Epoch 66: Train Loss = 1.5133, Val Loss = 1.9017\n",
      "Epoch 67: Train Loss = 1.5128, Val Loss = 1.9014\n",
      "Epoch 68: Train Loss = 1.5068, Val Loss = 1.9038\n",
      "Epoch 69: Train Loss = 1.5042, Val Loss = 1.8947\n",
      "Epoch 70: Train Loss = 1.5012, Val Loss = 1.8957\n",
      "Epoch 71: Train Loss = 1.4998, Val Loss = 1.8985\n",
      "Epoch 72: Train Loss = 1.4953, Val Loss = 1.8942\n",
      "Epoch 73: Train Loss = 1.4940, Val Loss = 1.8964\n",
      "Epoch 74: Train Loss = 1.4903, Val Loss = 1.8904\n",
      "Epoch 75: Train Loss = 1.4867, Val Loss = 1.8925\n",
      "Epoch 76: Train Loss = 1.4852, Val Loss = 1.8954\n",
      "Epoch 77: Train Loss = 1.4834, Val Loss = 1.8923\n",
      "Epoch 78: Train Loss = 1.4758, Val Loss = 1.8926\n",
      "Epoch 79: Train Loss = 1.4766, Val Loss = 1.8909\n",
      "Epoch 80: Train Loss = 1.4770, Val Loss = 1.8893\n",
      "Epoch 81: Train Loss = 1.4728, Val Loss = 1.8912\n",
      "Epoch 82: Train Loss = 1.4734, Val Loss = 1.8905\n",
      "Epoch 83: Train Loss = 1.4679, Val Loss = 1.8884\n",
      "Epoch 84: Train Loss = 1.4690, Val Loss = 1.8866\n",
      "Epoch 85: Train Loss = 1.4648, Val Loss = 1.8878\n",
      "Epoch 86: Train Loss = 1.4647, Val Loss = 1.8882\n",
      "Epoch 87: Train Loss = 1.4656, Val Loss = 1.8880\n",
      "Epoch 88: Train Loss = 1.4587, Val Loss = 1.8874\n",
      "Epoch 89: Train Loss = 1.4561, Val Loss = 1.8834\n",
      "Epoch 90: Train Loss = 1.4499, Val Loss = 1.8852\n",
      "Epoch 91: Train Loss = 1.4489, Val Loss = 1.8861\n",
      "Epoch 92: Train Loss = 1.4518, Val Loss = 1.8833\n",
      "Epoch 93: Train Loss = 1.4513, Val Loss = 1.8812\n",
      "Epoch 94: Train Loss = 1.4475, Val Loss = 1.8844\n",
      "Epoch 95: Train Loss = 1.4483, Val Loss = 1.8818\n",
      "Epoch 96: Train Loss = 1.4438, Val Loss = 1.8815\n",
      "Epoch 97: Train Loss = 1.4400, Val Loss = 1.8828\n",
      "Epoch 98: Train Loss = 1.4396, Val Loss = 1.8799\n",
      "Epoch 99: Train Loss = 1.4375, Val Loss = 1.8810\n",
      "Epoch 100: Train Loss = 1.4351, Val Loss = 1.8825\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, loss_fn, optimizer)\n",
    "    val_loss = evaluate(model, val_loader, loss_fn)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "529cc402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8893\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_loader, loss_fn)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b4c2c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music_sample(model, loader, vocab, inv_vocab, num_tokens=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            break\n",
    "\n",
    "        input_seq = batch_x[0].unsqueeze(0)  # shape: [1, seq_len]\n",
    "        generated = input_seq.clone()\n",
    "\n",
    "        context_tokens = input_seq[:, :3]  # M:, L:, K:\n",
    "        rolling_window = input_seq[:, 3:]  # actual melody\n",
    "\n",
    "        for _ in range(num_tokens):\n",
    "            rolling_window = generated[:, -WINDOW_SIZE:]  # keep last notes only\n",
    "            input_window = torch.cat((context_tokens, rolling_window), dim=1)\n",
    "            \n",
    "            output = model(input_window)\n",
    "            next_token = torch.argmax(output, dim=-1)\n",
    "            next_token = next_token.unsqueeze(1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "        generated_tokens = [inv_vocab[idx.item()] for idx in generated[0]]\n",
    "        abc_sequence = ''.join(generated_tokens)\n",
    "\n",
    "    return abc_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "254eb0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:4/4L:1/4K:DD/2|F/2AF/2A3/2A/2|\"G\"B/2A/2B/2c/2\"D\"d/2B/2A/2G/2|\"Em\"FE\"A7\"EF/2G/2|\"D\"A/2B/2A/2F/2Ad|\"D\"Add/2e/2f/2g/2|\"D\"afdf|\"A7\"edcB|\"D\"AFAd|\"A7\"cAAA|\"D\"FA\"A7\"AF|\"D\"FAAA|\"D\"dAFA|\"D\"dfdf|\"Em\"efed|\"A7\"cBAG|\"D\"FA\"A7\"AF|\"D\"FAdA\n"
     ]
    }
   ],
   "source": [
    "abc = generate_music_sample(model, test_loader, vocab, inv_vocab)\n",
    "print(abc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
