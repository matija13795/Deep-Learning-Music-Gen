{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad4a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_hyperparams(embed_dim, hidden_dim, learning_rate, dropout, weight_decay, epochs=20, patience=5):\n",
    "    model = MelodyModel(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=embed_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, loss_fn, optimizer)\n",
    "        val_loss = evaluate(model, val_loader, loss_fn)\n",
    "        print(f\"[Epoch {epoch+1}] Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "                break\n",
    "\n",
    "    return train_loss, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "search_space = {\n",
    "    \"embed_dim\": [64, 128],\n",
    "    \"hidden_dim\": [256, 512],\n",
    "    \"learning_rate\": [5e-4, 1e-3, 5e-3],\n",
    "    \"dropout\": [0.4, 0.5],\n",
    "    \"weight_decay\": [1e-4, 5e-4, 1e-5]\n",
    "}\n",
    "\n",
    "def sample_hyperparams(space):\n",
    "    return {\n",
    "        \"embed_dim\": random.choice(space[\"embed_dim\"]),\n",
    "        \"hidden_dim\": random.choice(space[\"hidden_dim\"]),\n",
    "        \"learning_rate\": random.choice(space[\"learning_rate\"]),\n",
    "        \"dropout\": random.choice(space[\"dropout\"]),\n",
    "        \"weight_decay\": random.choice(space[\"weight_decay\"]),\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "for trial in range(20):\n",
    "    print(f\"\\nTrial {trial+1}\")\n",
    "    params = sample_hyperparams(search_space)\n",
    "    print(\"Params:\", params)\n",
    "\n",
    "    train_loss, val_loss = train_with_hyperparams(**params)\n",
    "\n",
    "    results.append({\n",
    "        \"Trial\": trial + 1,\n",
    "        **params,\n",
    "        \"Train Loss\": train_loss,\n",
    "        \"Val Loss\": val_loss\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c163c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = sorted(results, key=lambda x: x[\"Val Loss\"])\n",
    "print(\"\\nTop Results:\")\n",
    "for r in sorted_results[:5]:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
