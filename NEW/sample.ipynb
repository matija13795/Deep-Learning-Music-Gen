{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f5f5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Set, Union\n",
    "from tokenizer import ABCTokenizer\n",
    "\n",
    "\n",
    "def sample(\n",
    "    model: nn.Module,\n",
    "    tokenizer: ABCTokenizer,\n",
    "    *,\n",
    "    max_len: int = 1024,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.9,\n",
    "    force_tokens: Set[str] = {\"|\", \"|:\", \":|\", \"::\", \"||\"},\n",
    "    force_threshold: float = 0.55,\n",
    "    device: Union[str, torch.device] = \"cpu\",\n",
    "    prime_str: str = \"L:1/16\\nM:4/4\\nK:Amin\\n\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate an ABC tune with nucleus sampling while forcing arg-max\n",
    "    for certain syntax-critical tokens (e.g. barlines) whenever the model\n",
    "    already assigns them >= `force_threshold` probability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    force_tokens : set[str] or None\n",
    "        Strings that should be taken greedily when the model is confident\n",
    "        enough.\n",
    "    force_threshold : float\n",
    "        Probability above which we override randomness and pick the token.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    ids = tokenizer.encode(prime_str)\n",
    "    inp = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    _, h = model(inp, None, return_hidden=True) # warm‑up hidden state\n",
    "    cur_id = ids[-1]\n",
    "    generated = ids.copy()\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor([[cur_id]], device=device)\n",
    "        logits, h = model(x, h,  return_hidden=True)  # shape [1,1,V]\n",
    "        logits = logits[0, 0] / temperature           # scale “spikiness”\n",
    "\n",
    "        # Soft‑max -> probabilities\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # -------- 1. FORCE‑ARGMAX ---------------------------------------\n",
    "        # If one of the force_tokens exceeds threshold, pick it directly.\n",
    "        forced = None\n",
    "        for tok in force_tokens:\n",
    "            tid = tokenizer.stoi.get(tok)\n",
    "            if tid is not None and probs[tid] >= force_threshold:\n",
    "                forced = tid\n",
    "                break\n",
    "        if forced is not None:\n",
    "            next_id = forced\n",
    "\n",
    "        # -------- 2. NUCLEUS SAMPLING -----------------------------------\n",
    "        else:\n",
    "            sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "            cumsum = torch.cumsum(sorted_probs, dim=0)\n",
    "            cutoff = (cumsum > top_p).nonzero(as_tuple=False)\n",
    "            last = cutoff[0, 0] + 1 if cutoff.numel() else len(sorted_idx)\n",
    "            pool_probs = sorted_probs[:last]\n",
    "            pool_idx = sorted_idx[:last]\n",
    "            pool_probs /= pool_probs.sum() # renormalise\n",
    "            next_id = int(pool_idx[torch.multinomial(pool_probs, 1)])\n",
    "\n",
    "        generated.append(next_id)\n",
    "        cur_id = next_id\n",
    "\n",
    "        # stop after blank line ends tune body\n",
    "        if tokenizer.itos[cur_id] == \"\\n\" and len(generated) > len(ids) + 800:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6e203470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json\n",
    "from torch.utils.data import random_split\n",
    "from dataset   import LeadSheetDataset, collate_fn\n",
    "\n",
    "# Read raw ABC file\n",
    "with open(\"leadsheets.abc\", \"r\") as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "with open(\"cache/abc_maps.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    maps = json.load(f)\n",
    "\n",
    "chord_map  = maps[\"chord_map\"]\n",
    "header_map = maps[\"header_map\"]\n",
    "inline_map = maps[\"inline_map\"]\n",
    "\n",
    "tunes = raw_data.strip().split(\"\\n\\n\")\n",
    "tokenizer = ABCTokenizer(chord_map=chord_map, header_map=header_map, inline_hdr_map=inline_map)\n",
    "tokenizer.build_vocab(tunes)\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "dataset = LeadSheetDataset(tunes, tokenizer)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "pad_idx = tokenizer.stoi[tokenizer.pad_token]\n",
    "collate = lambda batch: collate_fn(batch, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bd7bd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (embed): Embedding(215, 512, padding_idx=0)\n",
      "  (lstm): LSTM(512, 1024, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (fc_out): Linear(in_features=1024, out_features=215, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "Number of parameters: 15,026,903\n",
      "\n",
      "GRUModel(\n",
      "  (embed): Embedding(215, 512, padding_idx=0)\n",
      "  (gru): GRU(512, 1024, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc_out): Linear(in_features=1024, out_features=215, bias=True)\n",
      ")\n",
      "Numbers of parameters: 11,352,791\n"
     ]
    }
   ],
   "source": [
    "from models import LSTMModel, GRUModel\n",
    "\n",
    "# Load the model architecture\n",
    "lstm_model = LSTMModel(embed_dim=512, hidden_dim=1024, vocab_size=vocab_size, num_layers=2, dropout=0.3).to(\"cpu\")\n",
    "gru_model = GRUModel(embed_dim=512, hidden_dim=1024, num_layers=2, vocab_size=vocab_size, dropout=0.2).to(\"cpu\")\n",
    "\n",
    "# Load the checkpoint files\n",
    "checkpoint_lstm = torch.load('saved_models/LSTM_model.pt', map_location=\"cpu\", weights_only=True)\n",
    "checkpoint_gru  = torch.load('saved_models/GRU_model.pt', map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "# Load the state dicts\n",
    "lstm_model.load_state_dict(checkpoint_lstm['model_state_dict'])\n",
    "gru_model.load_state_dict(checkpoint_gru)\n",
    "\n",
    "lstm_total_params = sum(p.numel() for p in lstm_model.parameters())\n",
    "gru_total_params = sum(p.numel() for p in gru_model.parameters())\n",
    "print(lstm_model)\n",
    "print(f\"Number of parameters: {lstm_total_params:,}\")\n",
    "print()\n",
    "print(gru_model)\n",
    "print(f\"Numbers of parameters: {gru_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77c7baf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GRU Sample ===\n",
      "L:1/4\n",
      "K:C\n",
      "[K:G] D/F/ |\"G\" G B c/B/ |\"D\" A2 D |\"G\" B G B |\"D\" c A A |\"G\" B G G |\"Em\" B2 c/B/ |\"Am\" A c B |\"D\" A2 D | \n",
      "\"G\" G B c |\"G\" d2 B |\"Am\" A c B |\"D\" A2 d |\"Am\" c B A |\"D\" d c B |\"G\" G2 B |\"D\" A G E | \n",
      "\"Am\" A2 E/G/ |\"D\" F D d |\"G\" B G G |\"D\" A D d |\"G\" B G G |\"C\" c2 B |\"Am\" c2 E |\"D7\" D2 G |\"G\" G3 |] \n",
      " B/c/ |\"G\" d B/c/ d |\"C\" e c/e/ g |\"C\" c e g |\"D\" f e d |\"Am\" e c/B/ A |\"C\" e d/c/ B |\"Am\" A3 |\"D\" A3 | \n",
      "\"D\" d e f |\"G\" g d B |\"Am\" e d c |\"Em\" B/A/ G E |\"Am\" A G E |\"D\" D3 |\"G\" D G B |\"Am\" c A B | \n",
      "\"Am\" c B/A/ G/F/ |\"Em\" E3 |]\"C\" e f g |\"G\" d c/B/ A/G/ |\"Am\" A/B/ c/B/ c/d/ |\"D7\" e d c | \n",
      "\"G\" B A G |\"Em\" E3 |\"Am\" E A G |\"D\" F D d |\"G\" d B/c/ d/c/ |\"Em\" B G G |\"Am\" c e e |\"D\" d3 | \n",
      "\"G\" B G B |\"Am\" c A c |\"D\" d2 B/c/ |\"G\" d3 |\"G\" B G B |\"G\" d c B |\"Am\" A2 G/E/ |\"D7\" D E F |\"G\" G3 | \n",
      "\"G\" G3 |][M:3/4][K:D]\"D\" A d e | f d f |\"A\" e c A | e3 | A c e | A3 | \n",
      " A A c | e c A | a c A |\"G\" B3 |\"A\" e f e | A c e |\"G\" g b g |\"D\" f a f |\"A7\" e2 g |\"D\" f3 |] \n",
      " |:\"D\" a2 a | a b a |\"G\" g2 g |\"D\" a b a |\"A\" c e a | g e c |\"D\" d3 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final sample GRU\n",
    "print(\"\\n=== GRU Sample ===\")\n",
    "print(sample(gru_model, tokenizer, device=\"cpu\", prime_str = '''L:1/4\\n''', temperature=0.9, top_p=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5892b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LSTM Sample ===\n",
      "L:1/8\n",
      "M:3/4\n",
      "K:Amin\n",
      " E |\"Am\" A2 A B3 | c2 B2 A2 |\"E\" e4 e2 | e4 e2 | e2 f3 e | g2 f2 e2 | e6 | e4 E2 | \n",
      "\"Am\" A2 A3 B | c3 d e2 | d2 c3 B | A2 c3 A |\"E\" B3 A ^G2 |\"Am\" A3 c e2 | e2 a3 b | c'2 ba gf | \n",
      "\"Am\" e2 d2 c2 | e4 c'2 | e'3 d' c'2 | a2 ^ga ba | c'2 b3 a | g2 e3 c |\"Dm\" B2 A2 d2 | \n",
      "\"E\" e4 eB ||\"Am\" e3 d c2 | A4 EE | c4 Ac |\"E\" B4 E2 | G4 AB |\"Am\" c4 c2 | d2 e3 ^f | g6 | \n",
      "\"E\" a4 g2 | f2 e2 d2 |\"Am\" c6- | c2 d2 e2 |\"Dm\" f6 | f2 e2 d2 |\"Am\" e4 c2 | A4 |] E2 | \n",
      "\"Am\" E2 A2 ^G2 | A2 c2 c2 |\"E\" B3 E cB |\"Am\" A4 E2 | A3 B c2 |\"Dm\" d4 c2 |\"E\" B6- | B4 ee | \n",
      "\"E\" e2 ^d2 e2 |\"Am\" A4 c2 |\"Dm\" B3 A ^GB |\"E\" A2 ^G2 E2 |\"Am\" A6- | A2 z2 |] E2 |\"Am\" E4 A2 | \n",
      " c4 e2 |\"E\" B4 E2 | ^G4 E2 |\"Am\" A6- | A2 z2 ee |\"Dm\" d3 c B2 | A4 B2 |\"Am\" c6- | c2 c3 B | \n",
      "\"Dm\" A2 f2 d2 | B3 A B2 |\"Am\" c3 d c2 |\"E\" B4 E2 |\"Am\" A6 | A4 || c2 |\"Am\" e3 d c2 | A4 A2 | c4 e2 | \n",
      "\"E\" B4 e2 |\"Am\" c3 A E2 | c4 B2 |\"Dm\" A3 B c2 | d2 ^c2 d2 |\"E\" e4- eB |\"Am\" A4 || AB | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final sample LSTM\n",
    "print(\"\\n=== LSTM Sample ===\")\n",
    "print(sample(lstm_model, tokenizer, device=\"cpu\", prime_str = '''L:1/8\\nM:3/4\\nK:Amin\\n''', temperature=1.0, top_p=0.9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
