{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71f675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "_BAR_TOKENS = [\"||\", \"|]\", \"|:\", \":|\", \"::\", \"|\"]\n",
    "_ACCIDENTALS = [\"^^\", \"__\", \"^\", \"_\", \"=\"]\n",
    "_RESTS = {\"z\", \"Z\", \"x\"}\n",
    "_NOTE_LETTERS = set(\"ABCDEFGabcdefg\")\n",
    "_HDR_RE = re.compile(r'^(?:K|M|L):[^\\n]*\\n', re.MULTILINE)\n",
    "_INLINE_HDR_RE = re.compile(r'\\[(?:K|M|L):[^\\]]+\\]') # [K:C], [M:3/4] ...\n",
    "_TUPLET_RE = re.compile(r\"\\(\\d+\")                    # (3   (5   etc.\n",
    "_CHORD_RE = re.compile(\n",
    "    r'^\"'\n",
    "    r'(?P<root>[A-G](?:b|#)?)'          # root note\n",
    "    r'(?P<body>[0-9A-Za-z+\\-#]*)'       # quality / extensions\n",
    "    r'(?:/(?P<bass>[A-G](?:b|#)?))?'    # optional slash bass\n",
    "    r'\"$'\n",
    ")\n",
    "_SKIP_Q_RE = re.compile(r'\\[Q:[^\\]]+\\]')\n",
    "_DENOM_POW2 = r'(?:2|4|8|16)' # power-of-two denominators up to 16\n",
    "_DUR_RE = re.compile(\n",
    "    rf'''\n",
    "    (?:[1-9]\\d?/{_DENOM_POW2})  |  # 3/2  7/8  12/16\n",
    "    (?:/)  |      # / \n",
    "    (?:[2-8]|16)  # 2-8 or 16\n",
    "    ''',\n",
    "    re.VERBOSE\n",
    ")\n",
    "_SQUARE_BRACKET = re.compile(r\"\\[[^\\]]+\\]\")\n",
    "\n",
    "def build_token_type_counter(tunes: List[str], tokenizer, re) -> Counter:\n",
    "    \"\"\"Tokenise every tune once and count tokens of re's type.\"\"\"\n",
    "    counter = Counter()\n",
    "    for t in tunes:\n",
    "        for tok in tokenizer.tokenize_abc(t):\n",
    "            if re.fullmatch(tok):\n",
    "                counter[tok.strip()] += 1\n",
    "    return counter\n",
    "\n",
    "def build_chord_counter(tunes: List[str], tokenizer, re) -> Counter:\n",
    "    \"\"\"Tokenise every tune once and count tokens of re's type.\"\"\"\n",
    "    counter = Counter()\n",
    "    for t in tunes:\n",
    "        for tok in tokenizer.tokenize_abc(t):\n",
    "            if re.fullmatch(tok):\n",
    "                counter[tok] += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "\n",
    "def _strip_slash(ch: str) -> str:\n",
    "    return re.sub(r'/[A-G](?:b|#)?(?=\")', \"\", ch)\n",
    "\n",
    "\n",
    "def _to_seventh(ch: str, m) -> str:\n",
    "    \"\"\"root → root7   or   rootm7   (keep minor flag)\"\"\"\n",
    "    root, body = m.group(\"root\"), m.group(\"body\")\n",
    "    if body.startswith((\"m\", \"min\")) and not body.startswith(\"maj\"):\n",
    "        return f'\"{root}m7\"'\n",
    "    return f'\"{root}7\"'\n",
    "\n",
    "\n",
    "def _to_triad(ch: str, m) -> str:\n",
    "    root, body = m.group(\"root\"), m.group(\"body\")\n",
    "    if body.startswith((\"m\", \"min\")) and not body.startswith(\"maj\"):\n",
    "        return f'\"{root}m\"'\n",
    "    if body.startswith((\"dim\", \"o\")):\n",
    "        return f'\"{root}dim\"'\n",
    "    if body.startswith((\"aug\", \"+\")):\n",
    "        return f'\"{root}aug\"'\n",
    "    return f'\"{root}\"' # major / no quality\n",
    "\n",
    "\n",
    "def make_chord_map(counter: Counter, min_count: int) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build {rare_chord: mapped_chord} using the layered fallback:\n",
    "      1. keep if common (count ≥ min_count)\n",
    "      2. drop slash bass\n",
    "      3. collapse to 7-chord\n",
    "      4. collapse to triad (major/minor/dim/aug)\n",
    "      5. root only\n",
    "      6. UNK_CHORD\n",
    "    \"\"\"\n",
    "    high_freq = {c for c, n in counter.items() if n >= min_count}\n",
    "    chord_map: Dict[str, str] = {}\n",
    "    for chord, cnt in counter.items():\n",
    "        if cnt >= min_count: # keep as-is\n",
    "            chord_map[chord] = chord\n",
    "            continue\n",
    "\n",
    "        m = _CHORD_RE.fullmatch(chord)\n",
    "        if not m: # shouldn’t happen\n",
    "            chord_map[chord] = '\"UNK_CHORD\"'\n",
    "            continue\n",
    "\n",
    "        # 1- strip slash bass\n",
    "        c1 = _strip_slash(chord) # \"D7/F#\" → \"D7\"\n",
    "        if c1 in high_freq:\n",
    "            chord_map[chord] = c1; continue\n",
    "\n",
    "        # 2- collapse to seventh\n",
    "        c2 = _to_seventh(chord, m) # \"Em9\" → \"Em7\"\n",
    "        if c2 in high_freq:\n",
    "            chord_map[chord] = c2; continue\n",
    "\n",
    "        # 3- collapse to triad quality\n",
    "        c3 = _to_triad(chord, m) # \"Cm11b13\" → \"Cm\"\n",
    "        if c3 in high_freq:\n",
    "            chord_map[chord] = c3; continue\n",
    "\n",
    "        # 4- root only\n",
    "        root_only = f'\"{m.group(\"root\")}\"' # \"F#m\" → \"F#\"\n",
    "        if root_only in high_freq:\n",
    "            chord_map[chord] = root_only; continue\n",
    "\n",
    "        # 5- unknown\n",
    "        chord_map[chord] = '\"UNK_CHORD\"'\n",
    "\n",
    "    # make sure the placeholder itself is mapped to itself\n",
    "    chord_map['\"UNK_CHORD\"'] = '\"UNK_CHORD\"'\n",
    "    return chord_map\n",
    "\n",
    "\n",
    "def make_header_map(counter: Counter, min_key: int = 100, min_meter: int = 100) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Collapse rare K:/M: header *lines* to K:RARE\\n, M:RARE\\n\n",
    "    Returns {rare_header_line: mapped_header_line}.\n",
    "    \"\"\"\n",
    "    hdr_map: Dict[str, str] = {}\n",
    "    for hdr, n in counter.items():\n",
    "        if hdr.startswith(\"K:\"):\n",
    "            if n >= min_key:\n",
    "                hdr_map[hdr] = hdr \n",
    "            else:\n",
    "                hdr_map[hdr] = \"K:RARE\\n\"\n",
    "        elif hdr.startswith(\"M:\"):\n",
    "            if n >= min_meter:\n",
    "                hdr_map[hdr] = hdr\n",
    "            else:\n",
    "                hdr_map[hdr] =  \"M:RARE\\n\" \n",
    "\n",
    "    # ensure placeholders map to themselves so they stay printable\n",
    "    hdr_map.update({\"K:RARE\\n\":\"K:RARE\\n\",\n",
    "                    \"M:RARE\\n\":\"M:RARE\\n\"})\n",
    "    return hdr_map\n",
    "\n",
    "\n",
    "def inline_from_header_map(header_map: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Derive {inline_src: inline_tgt} from the full-line header_map.\n",
    "    full-line tokens look like  'K:D\\n'     (newline included)\n",
    "    inline tokens should look like '[K:D]'  (no newline)\n",
    "    \"\"\"\n",
    "    inline_map = {}\n",
    "    for full, mapped in header_map.items():\n",
    "        # strip the trailing newline from both src and dst\n",
    "        src_body = full.rstrip(\"\\n\")\n",
    "        dst_body = mapped.rstrip(\"\\n\")\n",
    "        inline_map[f\"[{src_body}]\"] = f\"[{dst_body}]\"\n",
    "\n",
    "    # ensure the placeholders map to themselves\n",
    "    for tag in (\"K:RARE\", \"M:RARE\"):\n",
    "        inline_map[f\"[{tag}]\"] = f\"[{tag}]\"\n",
    "\n",
    "    return inline_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a7b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import ABCTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read raw ABC file\n",
    "with open(\"leadsheets.abc\", \"r\") as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "tunes = raw_data.strip().split(\"\\n\\n\")\n",
    "raw_tok = ABCTokenizer()\n",
    "\n",
    "chord_counts = build_chord_counter(tunes, raw_tok, _CHORD_RE)\n",
    "hdr_counts = build_token_type_counter(tunes, raw_tok,  _HDR_RE)\n",
    "meter_counts = {k: v for k, v in hdr_counts.items() if k.startswith('M:')}\n",
    "key_counts   = {k: v for k, v in hdr_counts.items() if k.startswith('K:')}\n",
    "length_counts= {k: v for k, v in hdr_counts.items() if k.startswith('L:')}\n",
    "\n",
    "chord_df  = (pd.DataFrame(chord_counts.items(), columns=[\"chord\", \"count\"]).sort_values(\"count\", ascending=False))\n",
    "meter_df  = (pd.DataFrame(meter_counts.items(), columns=[\"meter\", \"count\"]).sort_values(\"count\", ascending=False))\n",
    "key_df  = (pd.DataFrame(key_counts.items(), columns=[\"key\", \"count\"]).sort_values(\"count\", ascending=False))\n",
    "length_df  = (pd.DataFrame(length_counts.items(), columns=[\"default_note_length\", \"count\"]).sort_values(\"count\", ascending=False))\n",
    "\n",
    "os.makedirs(\"token_frequencies\", exist_ok=True)\n",
    "chord_df.to_csv(\"token_frequencies/chord_frequencies.csv\",  index=False)\n",
    "meter_df.to_csv(\"token_frequencies/meter_frequencies.csv\",  index=False)\n",
    "key_df.to_csv(\"token_frequencies/key_frequencies.csv\",  index=False)\n",
    "length_df.to_csv(\"token_frequencies/length_frequencies.csv\",  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3880919",
   "metadata": {},
   "source": [
    "Based on the counts from the csv files, we determine the min_count for each category. That is, what is the minimum number of occurences which warrants a unique token. Everything that's smaller than min_count should be lumped into a \"RARE\" token. This is because such tokens will not have the opportunity to be properly learnt in training due to their infrequency. So, rather than polluting the vocabulary, we will either count them as \"RARE\" or even, when it makes sense (like in the case of chords), \"simplify\" them to popular tokens. Below we will cache this mapping for convenience of use in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e22675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maps cached to cache/abc_maps.json\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "chord_map    = make_chord_map(chord_counts, min_count=200)\n",
    "header_map   = make_header_map(hdr_counts, min_key=100, min_meter=100)\n",
    "inline_map   = inline_from_header_map(header_map)\n",
    "\n",
    "maps = {\n",
    "    \"chord_map\":  chord_map,\n",
    "    \"header_map\": header_map,\n",
    "    \"inline_map\": inline_map,\n",
    "}\n",
    "\n",
    "os.makedirs(\"cache\", exist_ok=True)\n",
    "with open(\"cache/abc_maps.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(maps, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Maps cached to cache/abc_maps.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd408294",
   "metadata": {},
   "source": [
    "Here's what our final vocabulary looks like (all of the tokens grouped by category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "637f9667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL VOCABULARY\n",
      "---------------\n",
      "vocab size → 215\n",
      "Accidental     : = ^ ^^ _ __\n",
      "Articulation   : - .\n",
      "Bar/Repeat     : :: :| | |: |] ||\n",
      "Broken         : < >\n",
      "Chord          : \"A\" \"A/C#\" \"A/E\" \"A7\" \"Ab\" \"Ab/Eb\" \"Ab7\" \"Am\" \"Am/C\" \"Am7\" \"B\" \"B7\" \"Bb\" \"Bb/F\" \"Bb7\" \"Bbm\" \"Bm\" \"Bm7\" \"C\" \"C#\" \"C#7\" \"C#m\" \"C/E\" \"C/G\" \"C7\" \"Cm\" \"D\" \"D/A\" \"D/F#\" \"D7\" \"Db\" \"Dm\" \"Dm/F\" \"Dm7\" \"E\" \"E7\" \"Eb\" \"Eb/Bb\" \"Eb7\" \"Em\" \"Em7\" \"F\" \"F#\" \"F#/C#\" \"F#7\" \"F#m\" \"F/C\" \"F7\" \"Fm\" \"G\" \"G#m\" \"G/B\" \"G/D\" \"G7\" \"Gb\" \"Gm\" \"Gm/Bb\" \"Gm7\"\n",
      "Chord-like melodic groupings: [ ]\n",
      "Duration       : / 16 2 27/8 3 3/2 3/4 3/8 4 5 5/2 6 7 7/2 7/4 7/8 8 9/2 9/4\n",
      "Grace Notes    : { }\n",
      "Header         : 'K:RARE\\n' 'L:1/16\\n' 'L:1/4\\n' 'L:1/8\\n' 'M:RARE\\n'\n",
      "Inline Header Change: '[K:A#min]' '[K:A]' '[K:Ab]' '[K:Abmin]' '[K:Ador]' '[K:Amin]' '[K:Amix]' '[K:Bb]' '[K:Bblyd]' '[K:Bmin]' '[K:C#]' '[K:C#min]' '[K:C]' '[K:Cb]' '[K:Clyd]' '[K:Cmin]' '[K:D#min]' '[K:D]' '[K:Db]' '[K:Ddor]' '[K:Dmin]' '[K:Dmix]' '[K:E]' '[K:Eb]' '[K:Edor]' '[K:Emin]' '[K:F#]' '[K:F]' '[K:G#min]' '[K:G]' '[K:Gb]' '[K:Gdor]' '[K:Gmin]' '[K:RARE]' '[K:bass]' '[K:treble]' '[M:12/8]' '[M:16/8]' '[M:18/16]' '[M:2/2]' '[M:2/4]' '[M:3/2]' '[M:3/4]' '[M:3/8]' '[M:4/4]' '[M:6/4]' '[M:6/8]' '[M:7/8]' '[M:8/4]' '[M:9/8]' '[M:RARE]'\n",
      "Misc           : \"UNK_CHORD\" 1 K:A K:Ab K:Ador K:Amin K:Amix K:Bb K:Bmin K:C K:Cmin K:D K:Db K:Ddor K:Dmin K:Dmix K:E K:Eb K:Edor K:Emin K:F K:F# K:G K:Gdor K:Gmin K:none M:12/8 M:2/2 M:2/4 M:3/2 M:3/4 M:3/8 M:4/4 M:6/4 M:6/8 M:7/8 M:9/8\n",
      "Newline        : '\\n'\n",
      "Note           : A B C D E F G a b c d e f g\n",
      "Octave         : ' ,\n",
      "Rest           : x z\n",
      "Space          : ' '\n",
      "Tuplet         : (2 (3 (4 (5 (7\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ABCTokenizer(chord_map=chord_map, header_map=header_map, inline_hdr_map=inline_map)\n",
    "tokenizer.build_vocab(tunes)\n",
    "\n",
    "print(\"FINAL VOCABULARY\")\n",
    "print(\"---------------\")\n",
    "print(\"vocab size →\", tokenizer.vocab_size())\n",
    "tokenizer.print_grouped_tokens(tokenizer.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f64b9",
   "metadata": {},
   "source": [
    "Here's an example of how a song would be tokenized (what the model will \"see\" vs. what the original song looks like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd153959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL SONG:\n",
      "X:20\n",
      "L:1/4\n",
      "M:4/4\n",
      "K:Emin\n",
      "\"^*\"\"Em\" E2\"D\" D2 |\"C\" E2\"B7\" B,2 |\"^*\"\"Em\" E E\"D\" F F |\"G\" G A/G/\"B7\" F2 |\"^*\"\"Em\" B B\"D\" A A | \n",
      "\"C\" G A/G/\"B7\" F B, |\"^*\"\"Em\" E2\"D\" D F |\"^Last: E\"\"Em\" E4 |]\n",
      "\n",
      "WHAT THE MODEL SEES:\n",
      "L:1/4\n",
      "M:4/4K:Emin\"Em\" E2\"D\" D2 |\"C\" E2\"B7\" B,2 |\"Em\" E E\"D\" F F |\"G\" G A/G/\"B7\" F2 |\"Em\" B B\"D\" A A | \n",
      "\"C\" G A/G/\"B7\" F B, |\"Em\" E2\"D\" D F |\"Em\" E4 |]\n"
     ]
    }
   ],
   "source": [
    "TUNE_NUMBER = 3\n",
    "print(\"ORIGINAL SONG:\")\n",
    "print(tunes[TUNE_NUMBER])\n",
    "print()\n",
    "print(\"WHAT THE MODEL SEES:\")\n",
    "print(tokenizer.decode(tokenizer.encode(tunes[TUNE_NUMBER])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
